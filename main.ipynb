{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cefe16-be88-4081-8b17-935cde78138d",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d685bf85-5e35-4131-910e-43f089dd8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f9f3d3-1b0f-46a8-86fc-27e1db5411bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "405b353c-f405-4a57-8bf8-409563ee62da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 1.12.1+cu116\n",
      "CUDA 버전: 11.6\n",
      "CUDA 사용 가능: True\n",
      "cuDNN 사용 가능: True\n",
      "GPU 수: 1\n",
      "GPU 0 이름: NVIDIA GeForce RTX 2060\n",
      "cuda\n",
      "cuDNN 버전: 8302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 버전:\", torch.version.cuda)\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "print(\"cuDNN 사용 가능:\", torch.backends.cudnn.enabled)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 수:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i} 이름:\", torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없습니다. CPU를 사용합니다.\")\n",
    "\n",
    "print(device)\n",
    "print(\"cuDNN 버전:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec73548-cf0e-46d1-b18c-c0c0e4862022",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3762a98-8a7d-4350-a438-ede046bd4db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    #SR = 32000\n",
    "    SR = 16000\n",
    "    N_MFCC = 13\n",
    "    # Dataset\n",
    "    ROOT_FOLDER = './'\n",
    "    # Training\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    # Others\n",
    "    SEED = 42\n",
    "    \n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b605f-2976-46e1-b0a1-f8a24a043506",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c886c0-9786-430c-8707-d7ba6120079a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d69809-909e-4f7e-86f7-2481d24abe1f",
   "metadata": {},
   "source": [
    "## Train & Validation Split - 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "835dd5f6-4d2c-4575-8d04-120fa26f0476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                  path label\n",
      "0  RUNQPNJF  ./train/RUNQPNJF.ogg  real\n",
      "1  JFAWUOGJ  ./train/JFAWUOGJ.ogg  fake\n",
      "2  RDKEKEVX  ./train/RDKEKEVX.ogg  real\n",
      "3  QYHJDOFK  ./train/QYHJDOFK.ogg  real\n",
      "4  RSPQNHAO  ./train/RSPQNHAO.ogg  real\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "db03e3f1-b77f-48b3-be08-f18c0ad87310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55438, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09cebc7b-6ed5-46cc-b304-867b285cb911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0bad227-7d47-4266-a90b-5c921858f1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*, y: Optional[numpy.ndarray] = None, sr: float = 22050, S: Optional[numpy.ndarray] = None, n_mfcc: int = 20, dct_type: int = 2, norm: Optional[str] = 'ortho', lifter: float = 0, **kwargs: Any) -> numpy.ndarray\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(librosa.feature.mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4204c665-4508-4bc4-ad79-0fbd3d7bdf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44350it [1:07:12, 11.00it/s]\n",
      "11088it [16:48, 10.99it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_noise(data, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(data))\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    # Cast back to same data type\n",
    "    augmented_data = augmented_data.astype(type(data[0]))\n",
    "    return augmented_data\n",
    "\n",
    "def change_pitch(data, sampling_rate = CONFIG.SR, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data,  sr=CONFIG.SR, n_steps=pitch_factor)\n",
    "\n",
    "def stretch_time(data, stretch_factor=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate=stretch_factor)\n",
    "\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        path = row['path']\n",
    "        y, sr = librosa.load(path, sr=CONFIG.SR)\n",
    "        \n",
    "        if train_mode:\n",
    "            y = add_noise(y)\n",
    "            y = change_pitch(y)\n",
    "            y = stretch_time(y)\n",
    "        \n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return np.array(features), np.array(labels)\n",
    "    return np.array(features)\n",
    "\n",
    "# Augmented training data\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# Extract MFCC features for training and validation sets\n",
    "train_mfcc, train_labels = get_mfcc_feature(train, train_mode=True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, train_mode=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a017bba-078f-4808-a9d4-d392498781ef",
   "metadata": {},
   "source": [
    "## Data Pre-processing : MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "60e16665-8972-41d0-b515-2df1fa4911b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[265], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m features, labels\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[1;32m---> 23\u001b[0m train_mfcc, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_mfcc_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m val_mfcc, val_labels \u001b[38;5;241m=\u001b[39m get_mfcc_feature(val, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[265], line 4\u001b[0m, in \u001b[0;36mget_mfcc_feature\u001b[1;34m(df, train_mode)\u001b[0m\n\u001b[0;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m()):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# librosa패키지를 사용하여 wav 파일 load\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m], sr\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mSR)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# librosa패키지를 사용하여 mfcc 추출\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        \n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features\n",
    "    \n",
    "train_mfcc, train_labels = get_mfcc_feature(train, True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded0f96-daad-4563-bc5b-8ec7a0eeecc0",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "95a3bc5a-dd20-49be-8ae8-e6c1763b9484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape: (44350, 13)\n",
      "First MFCC feature shape: (13,)\n"
     ]
    }
   ],
   "source": [
    "# MFCC 형태 확인\n",
    "print(f\"MFCC shape: {np.array(train_mfcc).shape}\")\n",
    "print(f\"First MFCC feature shape: {np.array(train_mfcc[0]).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe8e0745-07b7-401b-9603-b200f86e6591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.mfcc[index], self.label[index]\n",
    "        return self.mfcc[index]\n",
    "   \n",
    "train_dataset = CustomDataset(train_mfcc, train_labels)\n",
    "val_dataset = CustomDataset(val_mfcc, val_labels)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "26f6bdad-4734-4daf-831f-571de6f7359a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "\n",
    "# Custom Dataset 클래스\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, labels):\n",
    "        self.mfcc = mfcc\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mfcc = self.mfcc[index]\n",
    "        label = self.labels[index]\n",
    "        # reshape to (seq_len, feature_dim)\n",
    "        mfcc = mfcc.reshape(-1, mfcc.shape[-1])\n",
    "        return mfcc, label\n",
    "\n",
    "train_dataset = CustomDataset(train_mfcc, train_labels)\n",
    "val_dataset = CustomDataset(val_mfcc, val_labels)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b40a0d-a2f1-4af8-bbea-60b4d98a449a",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0651927-61de-43ba-b697-e7631936c865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dim=128, output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b77dc8d9-6d70-4663-a843-8695bc28a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 40, 174])\n",
      "After conv1 and pool: torch.Size([1, 32, 20, 87])\n",
      "After conv2 and pool: torch.Size([1, 64, 10, 43])\n",
      "After flatten: torch.Size([1, 27520])\n"
     ]
    }
   ],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # fc1의 입력 크기를 일단 placeholder 값으로 설정합니다.\n",
    "        self.fc1 = nn.Linear(64 * 10 * 43, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        print(f\"After conv1 and pool: {x.shape}\")\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        print(f\"After conv2 and pool: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        print(f\"After flatten: {x.shape}\")\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 40, 174)  # 배치 크기 1, 채널 1, MFCC 크기 (40, 174)\n",
    "model = AudioCNN()\n",
    "output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87d73d-3297-4d72-bf99-15c6fdb1c6a9",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bac623-a77e-47f5-9d12-a6f6d19a9b84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 학습 루프\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mCONFIG\u001b[49m\u001b[38;5;241m.\u001b[39mN_EPOCHS):\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "for epoch in range(CONFIG.N_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # tqdm을 사용하여 진행률 막대 추가\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=num_batches, desc=f\"Epoch {epoch+1}/{CONFIG.N_EPOCHS}\")\n",
    "    \n",
    "    for i, (features, labels) in progress_bar:\n",
    "        features = features.unsqueeze(1).float().to(device)  # 채널 차원 추가 및 FloatTensor로 변환\n",
    "        labels = labels.float().to(device)  # FloatTensor로 변환\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 정확도 계산\n",
    "        predicted = (output.squeeze() > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        # 진행률 막대에 손실 값과 정확도 업데이트\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1), accuracy=accuracy)\n",
    "    \n",
    "    epoch_loss = running_loss / num_batches\n",
    "    print(f\"Epoch [{epoch+1}/{CONFIG.N_EPOCHS}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 평가 루프 (선택적)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in val_loader:\n",
    "        features = features.unsqueeze(1).float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        output = model(features)\n",
    "        predicted = (output.squeeze() > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973952e1-f6e9-46de-9511-767f9338e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6256bc-6478-46b3-a497-29d1000876ef",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9bf9aded-c3e7-49bf-a9c9-cc90ddfa95ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/462 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 96, 1, 13] to have 1 channels, but got 96 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[257], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleCNN()\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m CONFIG\u001b[38;5;241m.\u001b[39mLR)\n\u001b[1;32m----> 4\u001b[0m infer_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[121], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[256], line 11\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 96, 1, 13] to have 1 channels, but got 96 channels instead"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6578bcb8-725d-4266-b9bc-0fd2d2163aad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 462/462 [00:01<00:00, 245.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 840.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.51068] Val Loss : [0.39299] Val AUC : [0.90903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 462/462 [00:01<00:00, 265.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 871.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.36436] Val Loss : [0.35884] Val AUC : [0.93616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 462/462 [00:01<00:00, 270.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 853.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.31575] Val Loss : [0.29110] Val AUC : [0.95032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 462/462 [00:01<00:00, 274.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 818.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.28430] Val Loss : [0.27155] Val AUC : [0.95696]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 462/462 [00:01<00:00, 268.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 864.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.26676] Val Loss : [0.25988] Val AUC : [0.95934]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645f578-0bca-4df5-8b08-f5e1311e2817",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae1682-c0a6-4118-a894-ef399f4db772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for mfcc, label in train_loader:\n",
    "        mfcc, label = mfcc.float(), label.float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mfcc)\n",
    "        loss = criterion(output.squeeze(), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 평가 루프\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for mfcc, label in val_loader:\n",
    "        mfcc, label = mfcc.float(), label.float()\n",
    "        output = model(mfcc)\n",
    "        predicted = (output.squeeze() > 0.5).float()\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87d53fc7-b499-45c7-b70d-796fc29fbf48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [15:33, 53.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 521/521 [00:01<00:00, 262.26it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./test.csv')\n",
    "test_mfcc = get_mfcc_feature(test, False)\n",
    "test_dataset = CustomDataset(test_mfcc, None)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "preds = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94df178-ca71-4446-923b-dab3f146c336",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1bf7934-e645-40e4-b3f8-555b43eb8a87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "submit.to_csv('./submit_data_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3ec65-b3e5-4d7f-9563-d1e627011006",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "submit.to_csv('./submit_data_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7df4671-82a5-40ec-ab6c-9811894e62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[Sequence], Iterable[Sequence], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, pin_memory_device: str = '')\n"
     ]
    }
   ],
   "source": [
    "print(inspect.signature(DataLoader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "fakeaudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
