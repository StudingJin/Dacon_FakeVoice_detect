{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce166c8-65d6-41fb-ac5f-6615fa7971a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 1.12.1+cu116\n",
      "CUDA 버전: 11.6\n",
      "CUDA 사용 가능: True\n",
      "cuDNN 사용 가능: True\n",
      "GPU 수: 1\n",
      "GPU 0 이름: NVIDIA GeForce RTX 2060\n",
      "cuda\n",
      "cuDNN 버전: 8302\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 버전:\", torch.version.cuda)\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "print(\"cuDNN 사용 가능:\", torch.backends.cudnn.enabled)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 수:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i} 이름:\", torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없습니다. CPU를 사용합니다.\")\n",
    "\n",
    "print(device)\n",
    "print(\"cuDNN 버전:\", torch.backends.cudnn.version())\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    #SR = 16000\n",
    "    N_MFCC = 13\n",
    "    # Dataset\n",
    "    ROOT_FOLDER = './'\n",
    "    # Training\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 40\n",
    "    LR = 3e-4\n",
    "    # Others\n",
    "    SEED = 42\n",
    "    \n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df06c8e2-a16f-4028-8227-65c0782d6205",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      4\u001b[0m     y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m], sr\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mSR)\n\u001b[1;32m----> 5\u001b[0m     mfcc \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_MFCC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     mfcc_lengths\u001b[38;5;241m.\u001b[39mappend(mfcc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(mfcc_lengths)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\librosa\\feature\\spectral.py:1989\u001b[0m, in \u001b[0;36mmfcc\u001b[1;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mel-frequency cepstral coefficients (MFCCs)\u001b[39;00m\n\u001b[0;32m   1844\u001b[0m \n\u001b[0;32m   1845\u001b[0m \u001b[38;5;124;03m.. warning:: If multi-channel audio input ``y`` is provided, the MFCC\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;124;03m>>> fig.colorbar(img2, ax=[ax[1]])\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1988\u001b[0m     \u001b[38;5;66;03m# multichannel behavior may be different due to relative noise floor differences between channels\u001b[39;00m\n\u001b[1;32m-> 1989\u001b[0m     S \u001b[38;5;241m=\u001b[39m power_to_db(melspectrogram(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m   1991\u001b[0m M: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mfftpack\u001b[38;5;241m.\u001b[39mdct(S, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdct_type, norm\u001b[38;5;241m=\u001b[39mnorm)[\n\u001b[0;32m   1992\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n_mfcc, :\n\u001b[0;32m   1993\u001b[0m ]\n\u001b[0;32m   1995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lifter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1996\u001b[0m     \u001b[38;5;66;03m# shape lifter for broadcasting\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\librosa\\feature\\spectral.py:2130\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[1;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[0;32m   2008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmelspectrogram\u001b[39m(\n\u001b[0;32m   2009\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2010\u001b[0m     y: Optional[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   2021\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   2022\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute a mel-scaled spectrogram.\u001b[39;00m\n\u001b[0;32m   2023\u001b[0m \n\u001b[0;32m   2024\u001b[0m \u001b[38;5;124;03m    If a spectrogram input ``S`` is provided, then it is mapped directly onto\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;124;03m    >>> ax.set(title='Mel-frequency spectrogram')\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2130\u001b[0m     S, n_fft \u001b[38;5;241m=\u001b[39m \u001b[43m_spectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2131\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2142\u001b[0m     \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[0;32m   2143\u001b[0m     mel_basis \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mmel(sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\librosa\\core\\spectrum.py:2945\u001b[0m, in \u001b[0;36m_spectrogram\u001b[1;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m   2941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2942\u001b[0m         )\n\u001b[0;32m   2943\u001b[0m     S \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2944\u001b[0m         np\u001b[38;5;241m.\u001b[39mabs(\n\u001b[1;32m-> 2945\u001b[0m             \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2946\u001b[0m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2947\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2948\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2949\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2950\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2951\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2952\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2953\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2954\u001b[0m         )\n\u001b[0;32m   2955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m power\n\u001b[0;32m   2956\u001b[0m     )\n\u001b[0;32m   2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m S, n_fft\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\librosa\\core\\spectrum.py:388\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[0;32m    385\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    387\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m fft\u001b[38;5;241m.\u001b[39mrfft(\n\u001b[1;32m--> 388\u001b[0m         \u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    389\u001b[0m     )\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 길이 확인\n",
    "mfcc_lengths = []\n",
    "for _, row in df.iterrows():\n",
    "    y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "    mfcc_lengths.append(mfcc.shape[1])\n",
    "max_len = max(mfcc_lengths)\n",
    "print(f\"Max length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2932a043-a6ab-4696-a862-a53eae0469dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 함수들\n",
    "def add_noise(y, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(y))\n",
    "    augmented_data = y + noise_factor * noise\n",
    "    return augmented_data\n",
    "\n",
    "def change_pitch(y, sr=32000, pitch_factor=2.0):\n",
    "    return librosa.effects.pitch_shift(y, sr=32000, n_steps=pitch_factor)\n",
    "\n",
    "def stretch_time(y, rate=1.1):\n",
    "    return librosa.effects.time_stretch(y, rate=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08feae3a-f1fa-42a7-9dbb-990866f297ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 44350/44350 [29:34<00:00, 24.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 11088/11088 [07:27<00:00, 24.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# mfcc 추출\n",
    "def get_mfcc_feature(df, sr=22050, n_mfcc=13, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        # librosa 패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(row['path'], sr=sr)\n",
    "\n",
    "        # 데이터 증강 적용\n",
    "        if train_mode:\n",
    "            augmentations = [add_noise, change_pitch, stretch_time]\n",
    "            augmentation = random.choice(augmentations)\n",
    "            if augmentation == change_pitch:\n",
    "                y = augmentation(y, sr)\n",
    "            else:\n",
    "                y = augmentation(y)\n",
    "\n",
    "        # librosa 패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        features.append(mfcc)\n",
    "        \n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features\n",
    "\n",
    "# MFCC 추출\n",
    "train_mfcc, train_labels = get_mfcc_feature(train, train_mode=True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, train_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "639142a8-26c2-49ed-930c-8fac9c9ba080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding 을 통해 정규화 ,,평균값으로 정규화를 안해도 이 방식으로 가능\n",
    "max_len = 1209\n",
    "\n",
    "def pad_mfcc(mfcc, max_len):\n",
    "    current_len = mfcc.shape[1]\n",
    "    pad_width = max_len - current_len\n",
    "    \n",
    "    if pad_width < 0:\n",
    "        # 현재 길이가 최대 길이보다 긴 경우, 최대 길이로 자르기\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    else:\n",
    "        # 제로 패딩 적용 (constant 모드로 0으로 패딩)\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    \n",
    "    return mfcc\n",
    "    \n",
    "def pad_mfcc_list(mfcc_list, max_len):\n",
    "    padded_features = []\n",
    "    for mfcc in mfcc_list:\n",
    "        mfcc_padded = pad_mfcc(mfcc, max_len)\n",
    "        # 채널 차원 추가\n",
    "        mfcc_padded = np.expand_dims(mfcc_padded, axis=0)  # (1, n_mfcc, max_len) 형태로 변환\n",
    "        padded_features.append(mfcc_padded)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "275cfc32-df01-421c-bb70-46727806f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomDataset 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc_features, labels):\n",
    "        self.mfcc_features = mfcc_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = self.mfcc_features[idx]\n",
    "        mfcc = mfcc.squeeze(0)  # 불필요한 차원 제거\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return mfcc, label\n",
    "        return mfcc\n",
    "\n",
    "# 패딩 적용\n",
    "train_mfcc_padded = pad_mfcc_list(train_mfcc, max_len)\n",
    "val_mfcc_padded = pad_mfcc_list(val_mfcc, max_len)\n",
    "\n",
    "# Numpy 배열로 변환 및 채널 차원 추가\n",
    "train_mfcc_padded = np.array(train_mfcc_padded)  # 이미 채널 차원이 추가된 상태\n",
    "val_mfcc_padded = np.array(val_mfcc_padded)      # 이미 채널 차원이 추가된 상태\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# CustomDataset 및 DataLoader 준비\n",
    "train_dataset = CustomDataset(train_mfcc_padded, train_labels)\n",
    "val_dataset = CustomDataset(val_mfcc_padded, val_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8161df-dc24-4374-9031-5e3a2e88a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        # Assuming the input size after the convolutions and pooling is (1024, 4, 4) if the input size is appropriately large\n",
    "        self.fc1 = nn.Linear(1024 * (CONFIG.N_MFCC // 64) * (max_len // 64), 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, CONFIG.N_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.pool(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.pool(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.pool(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.pool(self.bn5(self.conv5(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.pool(self.bn6(self.conv6(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(-1, 1024 * (CONFIG.N_MFCC // 64) * (max_len // 64))\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout(torch.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3625520-ccf0-4ce0-aee9-40ec212434af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AudioCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AudioCNNLSTM, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(64 * 3, 128, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(128 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply CNN layers\n",
    "        batch_size = x.size(0)\n",
    "        x = self.cnn_layers(x)  # (batch_size, 64, 3, 302)\n",
    "        # print(f\"After CNN: {x.shape}\")\n",
    "        \n",
    "        # Reshape for LSTM layer\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch_size, 3, 302, 64)\n",
    "        x = x.reshape(batch_size, 302, 64 * 3)  # (batch_size, 302, 192)\n",
    "        # print(f\"After reshape: {x.shape}\")\n",
    "        \n",
    "        # Apply LSTM\n",
    "        x, _ = self.lstm(x)  # (batch_size, 302, 128*2)\n",
    "        \n",
    "        # Use the last time step's output for classification\n",
    "        x = x[:, -1, :]  # (batch_size, 128*2)\n",
    "        \n",
    "        # Apply fully connected layer\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "730b9f01-a397-4351-a102-ea08d09cbec9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ImprovedAudioCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(ImprovedAudioCNNLSTM, self).__init__()\n",
    "        \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            nn.Dropout(0.5)  # 드롭아웃 추가\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(256, 256, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.cnn_layers(x)\n",
    "        # print(f\"After CNN: {x.shape}\")  # CNN 레이어 후 데이터 형태와 크기 출력\n",
    "\n",
    "        # Calculate the new shape for the LSTM layer\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch_size, height, width, channels)\n",
    "        x = x.reshape(batch_size, x.size(1) * x.size(2), x.size(3))  # (batch_size, height*width, channels)\n",
    "        # print(f\"After reshape: {x.shape}\")  # Reshape 후 데이터 형태와 크기 출력\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        # print(f\"After LSTM: {x.shape}\")  # LSTM 레이어 후 데이터 형태와 크기 출력\n",
    "        \n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e7926ad-f02f-4be7-b344-fec318ee07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score  # AUC 점수 계산을 위한 임포트\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)  # BCEWithLogitsLoss는 시그모이드 활성화 포함\n",
    "\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CONFIG.N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for features, labels in tqdm(iter(train_loader), desc=f\"Training Epoch {epoch}/{CONFIG.N_EPOCHS}\"):\n",
    "            features = features.unsqueeze(1).float().to(device)  # 채널 차원 추가\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            # 레이블 값이 [0, 1] 범위 내에 있는지 확인\n",
    "            assert labels.min() >= 0 and labels.max() <= 1, \"레이블 값이 [0, 1] 범위를 벗어났습니다.\"\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # 정확도 계산\n",
    "            predicted = (torch.sigmoid(output.squeeze()) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch}], Train Loss: [{_train_loss:.5f}] Val Loss: [{_val_loss:.5f}] Val AUC: [{_val_score:.5f}] Accuracy: [{accuracy:.2f}%]')\n",
    "\n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# AUC 계산 함수\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "\n",
    "# 모델 검증 함수\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader), desc=\"Validating\"):\n",
    "            features = features.unsqueeze(1).float().to(device)  # 채널 차원 추가\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            # 레이블 값이 [0, 1] 범위 내에 있는지 확인\n",
    "            assert labels.min() >= 0 and labels.max() <= 1, \"레이블 값이 [0, 1] 범위를 벗어났습니다.\"\n",
    "\n",
    "            probs = model(features)\n",
    "\n",
    "            loss = criterion(probs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "            # 정확도 계산\n",
    "            predicted = (torch.sigmoid(probs.squeeze()) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        _val_loss = np.mean(val_loss)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "\n",
    "        # Calculate AUC score\n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "    return _val_loss, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9714eac3-7f3f-455e-8cd0-e4b2fa9f5682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/40:   0%|                                                                     | 0/462 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (256x1x151). Calculated output size: (256x0x75). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mLR)\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m----> 5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m레이블 값이 [0, 1] 범위를 벗어났습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36mAudioCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn4\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn5(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:162\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\_jit_internal.py:423\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (256x1x151). Calculated output size: (256x0x75). Output size is too small"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스화 및 학습\n",
    "model = AudioCNN()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_model = train(model, optimizer, train_loader, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b65a2d51-dda1-4163-a561-88665e538a38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   0%|                                                                     | 0/462 [00:01<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스화 및 학습\n",
    "model = ImprovedAudioCNNLSTM(num_classes=2)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_model = train(model, optimizer, train_loader, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00f8769-d15e-4ce6-8f37-bd383a658bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40:   0%|                                                                              | 0/462 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (256x1x151). Calculated output size: (256x0x75). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# FloatTensor로 변환\u001b[39;00m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36mAudioCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn4\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn5(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:162\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\_jit_internal.py:423\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fakeaudio\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (256x1x151). Calculated output size: (256x0x75). Output size is too small"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 학습 루프 전에 레이블 값 확인\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(CONFIG.N_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # tqdm을 사용하여 진행률 막대 추가\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=num_batches, desc=f\"Epoch {epoch+1}/{CONFIG.N_EPOCHS}\")\n",
    "    \n",
    "    for i, (features, labels) in progress_bar:\n",
    "        features = features.unsqueeze(1).float().to(device)  # 채널 차원 추가 및 FloatTensor로 변환\n",
    "        labels = labels.float().to(device)  # FloatTensor로 변환\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (output.squeeze() > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        # 진행률 막대에 손실 값과 정확도 업데이트\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1), accuracy=accuracy)\n",
    "    \n",
    "    epoch_loss = running_loss / num_batches\n",
    "    print(f\"Epoch [{epoch+1}/{CONFIG.N_EPOCHS}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 평가 루프 (선택적)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in val_loader:\n",
    "        features = features.unsqueeze(1).float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        output = model(features)\n",
    "        predicted = (output.squeeze() > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06e73b51-c3f7-4542-a225-9fe6b3489aa2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                   path\n",
      "0  TEST_00000  ./test/TEST_00000.ogg\n",
      "1  TEST_00001  ./test/TEST_00001.ogg\n",
      "2  TEST_00002  ./test/TEST_00002.ogg\n",
      "3  TEST_00003  ./test/TEST_00003.ogg\n",
      "4  TEST_00004  ./test/TEST_00004.ogg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [1:21:56<00:00, 10.17it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./test.csv')\n",
    "print(test.head())  # 데이터프레임의 첫 몇 줄을 출력하여 경로가 올바른지 확인\n",
    "test_mfcc = get_mfcc_feature(test, sr=22050, train_mode=False)\n",
    "test_dataset = CustomDataset(test_mfcc, None)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f2238-4837-4112-8434-c3116433122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 적용\n",
    "max_len = 1209  # 적절한 max_len 설정\n",
    "test_mfcc_padded = pad_mfcc_list(test_mfcc, max_len)\n",
    "\n",
    "# Numpy 배열로 변환\n",
    "test_mfcc_padded = np.array(test_mfcc_padded)\n",
    "\n",
    "# CustomDataset 및 DataLoader 정의\n",
    "test_dataset = CustomDataset(test_mfcc_padded, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "model = AudioCNNLSTM(num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 추론 함수\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(test_loader):\n",
    "            features = features.unsqueeze(1).float().to(device)  # 채널 차원 추가 및 FloatTensor로 변환\n",
    "            probs = model(features)\n",
    "            probs = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f9dd6-32ad-49eb-aa72-d6af040b5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "preds = inference(model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "submit.to_csv('./submit_CNNLSTMdeep_dataPlus_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154c3839-0ead-4bd7-aa65-67d775a1e4ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MFCC shape: (44350, 1, 13, 1209)\n",
      "Validation MFCC shape: (11088, 1, 13, 1209)\n",
      "Train labels shape: (44350, 2)\n",
      "Validation labels shape: (11088, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train MFCC shape: {train_mfcc_padded.shape}')\n",
    "print(f'Validation MFCC shape: {val_mfcc_padded.shape}')\n",
    "print(f'Train labels shape: {train_labels.shape}')\n",
    "print(f'Validation labels shape: {val_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064be52-6e6a-4aa2-976c-cb08016c13d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "fakeaudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
